---
title: "COMP2501 Assignment 2"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Requirements

**Submission deadline: Mar 21th, 2023 at 23:59.**

**Full mark of assignment 2: 33.**

For the following questions, please:

1. Replace all [Input here] places with your information or your answer.
2. Complete the code block by adding your own code to fulfill the requirements in each question. Please use the existing code block and do not add your own code block. Noting that please use `head()` to show the corresponding results if there are too many rows in them.

Please make sure your Rmd file is a valid Markdown document and can be successfully knitted.

For assignment submission, please knit your final Rmd file into a Word document, and submit both your **Rmd** file and the knitted **Microsoft Word** document file to Moodle. You get 0 score if 1) the Rmd file you submitted cannot be knitted, and 2) you have not submitted a Word document. For each visualization question, please make sure that the generated plot is shown in-place with the question and after the code block. 

---


## Name and UID

Name: [Input here]

UID: [Input here]

---


### Environmental setup 
You need to have the `datasets`, `tidyr`, `dplyr`, `rvest`, `stringr`, `lubridate`, `gutenbergr`, `tidytext`, `textdata` and `ggplot2` packages installed. If not yet, please run `install.packages(c("datasets", "tidyr", "dplyr", "rvest", "stringr", "lubridate", "gutenbergr", "tidytext", "textdata", "ggplot2"))` in your R environment.

```{r}
# Load the package.
library(datasets)
library(tidyr)
library(dplyr)
library(rvest)
library(stringr)
library(lubridate)
library(gutenbergr)
library(tidytext)
library(textdata)
library(ggplot2)

```


### 1. (3 points) Load the built-in `airquality` dataset and view its first 6 rows. 1) Reshape the dataset (named `airquality_long`) using the `pivot_longer` function to convert the variables `Ozone`, `Solar.R`, `Wind`, and `Temp` into a new column named `Measurement`, with corresponding values in a new column named `Value`. 2) Reshape the `airquality_long` dataset (named `airquality_unite`) using the `unite` function to combine the `Month` and `Day` columns (with `-` as a separator) into a new column named `Date`. Use `head()` to show the results of each sub-question. (hint: you may refer to this link for information: https://www.statology.org/pivot_longer-in-r/)

```{r}
library(tidyr)


```


### 2. (3 points) Join the following `customers` and `orders` data frames by `customer_id`, with different join function, including: `left_join`, `right_join`, `inner_join`, `full_join`, `semi_join`, `anti_join` (separately), and print the corresponding results (named `left_join_df`, `right_join_df`, `inner_join_df`, `full_join_df`, `semi_join_df` and `anti_join_df` respectively). (hint: https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/join, https://dplyr.tidyverse.org/reference/mutate-joins.html)

```{r}
customers <- data.frame(
  customer_id = c(1, 2, 3, 4, 5),
  customer_name = c("Alice", "Bob", "Charlie", "Dave", "Eve"),
  city = c("New York", "San Francisco", "Boston", "Seattle", "Chicago")
)
orders <- data.frame(
  customer_id = c(1, 1, 2, 2, 2, 3, 3, 4, 5),
  order_id = c(101, 102, 201, 202, 203, 301, 302, 401, 501),
  order_amount = c(100, 200, 150, 75, 225, 300, 225, 175, 250)
)


```


### 3. (2 points) Find the union, intersection and difference of the following `df1` and `df2` data frames, and print the corresponding results (named `union_df`, `intersect_df`, `setdiff_df_1_2` and `setdiff_df_2_1` respectively).

```{r}
df1 <- data.frame(id = c(1, 2, 3), value = c("a", "b", "c"))
df2 <- data.frame(id = c(3, 4, 5), value = c("c", "d", "e"))


```


### 4. (3 points) Scrape the 1) movie titles, 2) their ratings, and 3) release years from the IMDb Top Rated Movies webpage (https://www.imdb.com/chart/top/) with the `rvest` package. Store the data in a data frame (named `movies`) and print the top 10 observations in `movies`. (hint: https://jtr13.github.io/cc19/web-scraping-using-rvest.html)

```{r}
library(rvest)


```


### 5. (3 points) Using the `stringr` package in R, perform the following tasks: 1) Extract all the phone numbers from the following text: "Please call us at 123-456-7890 or 555-555-5555." 2) Extract all the email addresses from the following text: "Contact us at info@example.com or support@example.com." 3) Replace all the URLs (https://www.xxx.com) in the following text with the string "URL": "Check out our website at https://www.example.com and our blog at https://blog.example.com.". Print the corresponding results.

```{r}
library(stringr)


```


### 6. (2 points) Using the `lubridate` package in R, parse the `date_time` column in the `date_data` and create new columns for standard `date` and `time` components, and print the final results.

```{r}
library(lubridate)

date_data <- data.frame(date_time = c("2023-02-22 7:30:15", "2023-02-23 12:15:30", "2023-02-24 23:59:59"))


```


### 7. (17 points) Explore the advanced data wrangling with the `gutenbergr` package and its corresponding datasets, and answer the following questions.

#### a. (1 points) Install the `gutenbergr` package and load the `gutenberg_metadata` as `books`. Print the first 6 rows, the number of observations (rows) and variables (columns), and the names of all variables in `books`.

```{r}
library(gutenbergr)


```


#### b. (2 points) Remove any rows in `books` that have missing values in the `author` column, and then count the number of books for each author in a descending order. Who has the most publications and what's the exact numer (ignoring `Various` and `Anonymous` as an author name)?

```{r}


```


#### c. (2 points) Create a subset of `books` with only `Shakespeare, William`'s English publications, named `shakespeare_books`. Print the first 6 rows in `shakespeare_books`. 

```{r}


```


#### d. (4 points) Filter the dataset `shakespeare_books` to only include specifically the book `Hamlet` as `shakespeare_hamlet`, and extract only `gutenberg_id`, `title` and `author` columns to save, and if there are more that one observation in `shakespeare_hamlet`, just preserve the first observation with `slice()`. Then use `gutenberg_download()` to download the corresponding texts according to `shakespeare_hamlet$gutenberg_id` as `hamlet_text`. Lastly join `shakespeare_hamlet` and `hamlet_text` with `left_join()` as `hamlet_data`, and remove any missing values in the `text` column as well as convert the `text` column to lowercase.

```{r}


```


#### e. (4 points) Perform sentiment analysis on `hamlet_data` using the `tidytext` package. First get the sentiment lexicon `afinn` through `get_sentiments()` using the `textdata` package and store it in `hamlet_sentiments`. Then extract each token in `text` column of `hamlet_data` with `unnest_tokens()` and remove the stop words with `anti_join()`, and then join it with `hamlet_sentiments` by `inner_join`, and count the number of `word` and its sentiment `value` in a descending order, saved as `hamlet_words`. (hint: http://rafalab.dfci.harvard.edu/dsbook/text-mining.html#sentiment-analysis)

```{r}
library(tidytext)
library(textdata)


```


#### f. (4 points) Folloing question e, please do operations on a dataset copy of `hamlet_words` as `hamlet_top_words` to obtain the results with `group_by(value)` and `top_n()`, and reorder the results in a descending order of `n`, then create a bar plot with `geom_col()` of the top 1 most common positive and negative words in `hamlet_words`. Set an appropriate plot title and axis titles.

```{r}
library(ggplot2)


```
